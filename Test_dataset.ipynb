{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import torchvision\n",
    "import random\n",
    "\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "# Step 1: Prepare your dataset\n",
    "class SketchDataset(Dataset):\n",
    "    def __init__(self, csv_file, image_root_dir, sketch_root_dir, transform=None):\n",
    "        \n",
    "        self.data_frame = pd.read_csv(csv_file)\n",
    "        self.image_root_dir = image_root_dir\n",
    "        self.sketch_root_dir = sketch_root_dir\n",
    "        self.transform = transform\n",
    "        self.num_sketches = 3  # Update with the actual number of sketches\n",
    "        self.num_samples = len(self.data_frame)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # print(\"idx: \",idx)\n",
    "        img_name = os.path.join(self.image_root_dir, self.data_frame.iloc[idx, 0] + '.jpg')\n",
    "        sketch_idx = idx % self.num_sketches  # Cyclic indexing for sketches\n",
    "        # print(sketch_idx)\n",
    "        # sketch_name = os.path.join(self.sketch_root_dir, self.data_frame.iloc[idx, 0] + '_segmentation'+'.png')\n",
    "        sketch_name = os.path.join(self.sketch_root_dir, f\"sketch_{sketch_idx + 1}.png\")\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        sketch = Image.open(sketch_name).convert('RGB')\n",
    "\n",
    "        label = torch.tensor(self.data_frame.iloc[idx, 1:], dtype=torch.float32)\n",
    "\n",
    "        rand_idx= random.randint(0, self.num_samples-1)\n",
    "        rand_label = torch.tensor(self.data_frame.iloc[rand_idx, 1:], dtype=torch.float32)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            sketch = self.transform(sketch)\n",
    "        \n",
    "        return label, sketch, image,img_name, rand_label\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256,256)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Modify paths as needed\n",
    "# test_dataset = SketchDataset(csv_file='/home/cvlab/Karan/A_3/Dataset_A4/Test_data/Test_labels.csv', \n",
    "#                              image_root_dir='/home/cvlab/Karan/A_3/Dataset_A4/Test_data/Test_images',\n",
    "#                              sketch_root_dir='/home/cvlab/Karan/A_3/Dataset_A4/Test_data/Paired_test_sketch',\n",
    "#                              transform=transform)\n",
    "test_dataset = SketchDataset(csv_file='/home/cvlab/Karan/A_3/Dataset_A4/Test_data/Test_labels copy.csv', \n",
    "                             image_root_dir='/home/cvlab/Karan/A_3/Dataset_A4/Test_data/Test_images',\n",
    "                             sketch_root_dir='/home/cvlab/Karan/A_3/Dataset_A4/Test_data/Unpaired_sketch',\n",
    "                             transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, down=True, act=\"relu\", use_dropout=False):\n",
    "        super(Block, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 4, 2, 1, bias=False, padding_mode=\"reflect\")\n",
    "            if down\n",
    "            else nn.ConvTranspose2d(in_channels, out_channels, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU() if act == \"relu\" else nn.LeakyReLU(0.2),\n",
    "        )\n",
    "\n",
    "        self.use_dropout = use_dropout\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.down = down\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return self.dropout(x) if self.use_dropout else x\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, in_channels=10, features=64): # 3 earlier\n",
    "        super().__init__()\n",
    "        self.initial_down = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, features, 4, 2, 1, padding_mode=\"reflect\"),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "        self.down1 = Block(features, features * 2, down=True, act=\"leaky\", use_dropout=False)\n",
    "        self.down2 = Block(\n",
    "            features * 2, features * 4, down=True, act=\"leaky\", use_dropout=False\n",
    "        )\n",
    "        self.down3 = Block(\n",
    "            features * 4, features * 8, down=True, act=\"leaky\", use_dropout=False\n",
    "        )\n",
    "        self.down4 = Block(\n",
    "            features * 8, features * 8, down=True, act=\"leaky\", use_dropout=False\n",
    "        )\n",
    "        self.down5 = Block(\n",
    "            features * 8, features * 8, down=True, act=\"leaky\", use_dropout=False\n",
    "        )\n",
    "        self.down6 = Block(\n",
    "            features * 8, features * 8, down=True, act=\"leaky\", use_dropout=False\n",
    "        )\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Conv2d(features * 8, features * 8, 4, 2, 1), nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.up1 = Block(features * 8, features * 8, down=False, act=\"relu\", use_dropout=True)\n",
    "        self.up2 = Block(\n",
    "            features * 8 * 2, features * 8, down=False, act=\"relu\", use_dropout=True\n",
    "        )\n",
    "        self.up3 = Block(\n",
    "            features * 8 * 2, features * 8, down=False, act=\"relu\", use_dropout=True\n",
    "        )\n",
    "        self.up4 = Block(\n",
    "            features * 8 * 2, features * 8, down=False, act=\"relu\", use_dropout=False\n",
    "        )\n",
    "        self.up5 = Block(\n",
    "            features * 8 * 2, features * 4, down=False, act=\"relu\", use_dropout=False\n",
    "        )\n",
    "        self.up6 = Block(\n",
    "            features * 4 * 2, features * 2, down=False, act=\"relu\", use_dropout=False\n",
    "        )\n",
    "        self.up7 = Block(features * 2 * 2, features, down=False, act=\"relu\", use_dropout=False)\n",
    "        self.final_up = nn.Sequential(\n",
    "            nn.ConvTranspose2d(features * 2, 3, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        d1 = self.initial_down(x)\n",
    "        d2 = self.down1(d1)\n",
    "        d3 = self.down2(d2)\n",
    "        d4 = self.down3(d3)\n",
    "        d5 = self.down4(d4)\n",
    "        d6 = self.down5(d5)\n",
    "        d7 = self.down6(d6)\n",
    "        bottleneck = self.bottleneck(d7)\n",
    "        up1 = self.up1(bottleneck)\n",
    "        up2 = self.up2(torch.cat([up1, d7], 1))\n",
    "        up3 = self.up3(torch.cat([up2, d6], 1))\n",
    "        up4 = self.up4(torch.cat([up3, d5], 1))\n",
    "        up5 = self.up5(torch.cat([up4, d4], 1))\n",
    "        up6 = self.up6(torch.cat([up5, d3], 1))\n",
    "        up7 = self.up7(torch.cat([up6, d2], 1))\n",
    "        last = self.final_up(torch.cat([up7, d1], 1))\n",
    "        # print(\"last: \",last.shape)\n",
    "        return last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model\n",
    "model = Generator(in_channels=10, features=64)  # Replace YourModel with the actual model class\n",
    "# model.load_state_dict(torch.load('/home/cvlab/Karan/A_3/logs/CGAN_all/best_model.pth'))\n",
    "# model.load_state_dict(torch.load('A_3/A4_Codes/CGAN_new_data_paired/best_model.pth'))\n",
    "model.load_state_dict(torch.load('/home/cvlab/Karan/A_3/A4_Codes/CGAN_new_data/best_model.pth'))\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "# Load pre-trained model\n",
    "model1 = models.resnet18(pretrained=True)\n",
    "\n",
    "# Freeze model parameters\n",
    "for param in model1.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Modify the last fully connected layer to match the number of classes\n",
    "num_ftrs = model1.fc.in_features\n",
    "model1.fc = nn.Linear(num_ftrs, 7)  # Assuming 7 classes\n",
    "\n",
    "# Load the trained model state dictionary\n",
    "model1.load_state_dict(torch.load('/home/cvlab/Karan/A_3/A4_Codes/model_weights_new.pth'))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model1.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "from torchvision.models import inception_v3\n",
    "from scipy.stats import entropy\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Create a directory to save the images if it doesn't exist\n",
    "output_dir = 'Unpaired_final'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Open CSV file to save image names and labels\n",
    "csv_file = open('Unpaired_Final.csv', 'w', newline='')\n",
    "csv_writer = csv.writer(csv_file)\n",
    "csv_writer.writerow(['Image_Name', 'Label'])\n",
    "\n",
    "# Define a function to generate images from sketches and labels\n",
    "def generate_images(real_gen):\n",
    "    with torch.no_grad():\n",
    "        generated_images = model(real_gen)\n",
    "        # generated_images = torch.sigmoid(generated_images)  # If your model's output is not normalized\n",
    "    return generated_images.to(device)\n",
    "\n",
    "# Function to calculate the inception score\n",
    "def calculate_inception_score(images, batch_size=32, splits=1):\n",
    "    # Load Inception V3 model\n",
    "    model = inception_v3(pretrained=True, transform_input=True, aux_logits=True).to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Define transformations\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((299, 299)),\n",
    "    ])\n",
    "\n",
    "    # Calculate activations for real images\n",
    "    preds = []\n",
    "    for batch_start in tqdm(range(0, len(images), batch_size), desc=\"Calculating activations\"):\n",
    "        batch_images = images[batch_start:batch_start+batch_size]\n",
    "        batch_images = [preprocess(img) for img in batch_images]\n",
    "        batch_images = torch.stack(batch_images)\n",
    "        pred = model(batch_images).detach().cpu().numpy()\n",
    "        preds.append(pred)\n",
    "    preds = np.concatenate(preds)\n",
    "\n",
    "    # Calculate Inception Score\n",
    "    scores = []\n",
    "    for i in range(splits):\n",
    "        part = preds[i * (preds.shape[0] // splits): (i + 1) * (preds.shape[0] // splits), :]\n",
    "        p_yx = np.exp(part) / np.exp(part).sum(1, keepdims=True)\n",
    "        p_y = np.exp(np.mean(part, axis=0)) / np.exp(np.mean(part, axis=0)).sum()\n",
    "        scores.append(entropy(p_yx.T) + entropy(p_y))\n",
    "    return np.mean(scores), np.std(scores)\n",
    "\n",
    "# Compare generated images with real images and compute Inception Score\n",
    "inception_scores = []\n",
    "correct = 0\n",
    "total = 0\n",
    "# Initialize a dictionary to keep track of sampled images for each label\n",
    "sampled_images_per_label = {label: 0 for label in range(7)}\n",
    "batch_counter = 0\n",
    "\n",
    "for labels, sketches, real_images, img_names, rand_label in test_loader:\n",
    "    labels, sketches, real_images = labels.to(device), sketches.to(device), real_images.to(device)\n",
    "\n",
    "    img_size = 256\n",
    "    embed_gen = nn.Embedding(7, 1).to(device)\n",
    "    embedding_gen = embed_gen(labels.long())\n",
    "    embedding_gen = embedding_gen.unsqueeze(3)\n",
    "    upsampled_embedding_gen = F.interpolate(embedding_gen, size=(img_size, img_size), mode='nearest')\n",
    "    real_gen = torch.cat([sketches, upsampled_embedding_gen], dim=1)\n",
    "\n",
    "    generated_images = generate_images(real_gen)\n",
    "    generated_images_numpy = generated_images.cpu().detach().numpy()\n",
    "    real_images_numpy = real_images.cpu().detach().numpy()\n",
    "\n",
    "    # Calculate Inception Score between generated and real images\n",
    "    inception_score, _ = calculate_inception_score(np.concatenate([generated_images_numpy, real_images_numpy]))\n",
    "    inception_scores.append(inception_score)\n",
    "\n",
    "    # Save only 5 images from each batch, one sample for each label\n",
    "    for i in range(len(real_images)):\n",
    "        label = labels[i].nonzero().item()\n",
    "\n",
    "        outputs = model1(generated_images)\n",
    "        # Get the predicted classes\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        # Convert one-hot encoded labels to class indices\n",
    "        labels_indices = torch.argmax(labels, dim=1)\n",
    "        # Update number of correctly classified samples\n",
    "        correct += (predicted == labels_indices).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "        generated_img = transforms.ToPILImage()(generated_images[i].cpu())\n",
    "        real_img = transforms.ToPILImage()(real_images[i].cpu())\n",
    "        sketch = transforms.ToPILImage()(sketches[i].cpu())\n",
    "\n",
    "        # Display generated and real images side by side\n",
    "        comparison_img = Image.new('RGB', (generated_img.width , generated_img.height))\n",
    "        comparison_img.paste(generated_img)\n",
    "\n",
    "        # Save the comparison image with a unique filename based on batches\n",
    "        comparison_img_name = os.path.join(output_dir, f\"comparison_batch{batch_counter}_label{label}_sample{sampled_images_per_label[label]}.jpg\")\n",
    "        comparison_img.save(comparison_img_name)\n",
    "        \n",
    "        # Write image name and label to CSV\n",
    "        csv_writer.writerow([os.path.basename(comparison_img_name), label])\n",
    "        sampled_images_per_label[label] += 1  # Increment the count for this label\n",
    "\n",
    "# Close CSV file\n",
    "csv_file.close()\n",
    "\n",
    "# Calculate the accuracy over the entire test data\n",
    "accuracy = correct / total\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Print average Inception Score\n",
    "print(\"Average Inception Score:\", np.mean(inception_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torchvision.models import inception_v3\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from scipy.linalg import sqrtm\n",
    "\n",
    "# Define a function to generate images from sketches and labels\n",
    "def generate_images(real_gen):\n",
    "    with torch.no_grad():\n",
    "        generated_images = model(real_gen)\n",
    "        generated_images = torch.sigmoid(generated_images)  # If your model's output is not normalized\n",
    "    return generated_images.to(device)\n",
    "# Function to calculate the Frechet Inception Distance (FID)\n",
    "def calculate_fid(real_images, generated_images, batch_size=32):\n",
    "    # Load Inception V3 model\n",
    "    model = inception_v3(pretrained=True, transform_input=True, aux_logits=True).to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Define transformations\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((299, 299)),\n",
    "        # transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "    ])\n",
    "\n",
    "    # Calculate activations for real images\n",
    "    real_activations = []\n",
    "    for batch_start in tqdm(range(0, len(real_images), batch_size), desc=\"Calculating real activations\"):\n",
    "        batch_real_images = real_images[batch_start:batch_start+batch_size]\n",
    "        batch_real_images = torch.stack([preprocess(img) for img in batch_real_images])\n",
    "        \n",
    "        # conv = nn.Conv2d(256, 3, kernel_size=1)\n",
    "        # batch_real_images = conv(batch_real_images)\n",
    "\n",
    "        batch_real_activations = model(batch_real_images).detach().cpu().numpy()\n",
    "        real_activations.append(batch_real_activations)\n",
    "    real_activations = np.concatenate(real_activations)\n",
    "\n",
    "    # Calculate activations for generated images\n",
    "    generated_activations = []\n",
    "    for batch_start in tqdm(range(0, len(generated_images), batch_size), desc=\"Calculating generated activations\"):\n",
    "        batch_generated_images = generated_images[batch_start:batch_start+batch_size]\n",
    "        batch_generated_images = torch.stack([preprocess(img) for img in batch_generated_images])\n",
    "\n",
    "        conv = nn.Conv2d(256, 3, kernel_size=1)\n",
    "        batch_generated_images = conv(batch_generated_images)\n",
    "\n",
    "\n",
    "        batch_generated_activations = model(batch_generated_images).detach().cpu().numpy()\n",
    "        generated_activations.append(batch_generated_activations)\n",
    "    generated_activations = np.concatenate(generated_activations)\n",
    "\n",
    "    # Calculate mean and covariance for real and generated activations\n",
    "    mu_real = np.mean(real_activations, axis=0)\n",
    "    mu_generated = np.mean(generated_activations, axis=0)\n",
    "    sigma_real = np.cov(real_activations, rowvar=False)\n",
    "    sigma_generated = np.cov(generated_activations, rowvar=False)\n",
    "\n",
    "    # Calculate FID\n",
    "    diff = mu_real - mu_generated\n",
    "    cov_sqrt = sqrtm(sigma_real.dot(sigma_generated))\n",
    "    if np.iscomplexobj(cov_sqrt):\n",
    "        cov_sqrt = cov_sqrt.real\n",
    "    fid = np.sum(diff**2) + np.trace(sigma_real + sigma_generated - 2 * cov_sqrt)\n",
    "    return fid\n",
    "\n",
    "# Compare generated images with real images and compute FID\n",
    "fid_scores = []\n",
    "for labels, sketches, real_images, img_names, rand_label in test_loader:\n",
    "    labels, sketches, real_images = labels.to(device), sketches.to(device), real_images.to(device)\n",
    "\n",
    "    img_size = 256\n",
    "    embed_gen = nn.Embedding(7, 1).to(device)\n",
    "    embedding_gen = embed_gen(labels.long())\n",
    "    embedding_gen = embedding_gen.unsqueeze(3)\n",
    "    upsampled_embedding_gen = F.interpolate(embedding_gen, size=(img_size,img_size), mode='nearest')\n",
    "    real_gen = torch.cat([sketches, upsampled_embedding_gen], dim=1)\n",
    "\n",
    "    generated_images = generate_images(real_gen)\n",
    "    generated_images_numpy = generated_images.cpu().detach().numpy()\n",
    "    real_images_numpy = real_images.cpu().detach().numpy()\n",
    "\n",
    "    # Calculate FID between generated and real images\n",
    "    fid_score = calculate_fid(real_images_numpy, generated_images_numpy)\n",
    "    fid_scores.append(fid_score)\n",
    "\n",
    "# Print average FID\n",
    "print(\"Average FID:\", np.mean(fid_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "idf1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
