{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import torchvision\n",
    "import random\n",
    "import wandb\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class SketchDataset(Dataset):\n",
    "    def __init__(self, csv_file, image_root_dir, sketch_root_dir, transform=None):\n",
    "        \n",
    "        self.data_frame = pd.read_csv(csv_file)\n",
    "        self.image_root_dir = image_root_dir\n",
    "        self.sketch_root_dir = sketch_root_dir\n",
    "        self.transform = transform\n",
    "        self.num_sketches = len(self.data_frame)  # Update with the actual number of sketches\n",
    "        self.num_samples = len(self.data_frame)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # print(\"idx: \",idx)\n",
    "        img_name = os.path.join(self.image_root_dir, self.data_frame.iloc[idx, 0] + '.jpg')\n",
    "        # sketch_idx = idx % self.num_sketches  # Cyclic indexing for sketches\n",
    "        # print(sketch_idx)\n",
    "        sketch_name = os.path.join(self.sketch_root_dir, self.data_frame.iloc[idx, 0] + '_segmentation'+'.png')\n",
    "        \n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        sketch = Image.open(sketch_name).convert('RGB')\n",
    "\n",
    "        label = torch.tensor(self.data_frame.iloc[idx, 1:], dtype=torch.float32)\n",
    "\n",
    "        rand_idx= random.randint(0, self.num_samples-1)\n",
    "        rand_label = torch.tensor(self.data_frame.iloc[rand_idx, 1:], dtype=torch.float32)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            sketch = self.transform(sketch)\n",
    "        \n",
    "        return label, sketch, image, img_name, rand_label\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256,256)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Modify paths as needed\n",
    "train_dataset = SketchDataset(csv_file = '/home/cvlab/Karan/A_3/Dataset_A4/Train_labels.csv', \n",
    "                              image_root_dir = '/home/cvlab/Karan/A_3/Dataset_A4/Train_data',\n",
    "                              sketch_root_dir = '/home/cvlab/Karan/A_3/Dataset_A4/Paired_train_sketches',\n",
    "                              transform = transform)\n",
    "dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Function to display a batch of images\n",
    "def show_images(images, titles=None):\n",
    "    num_images = len(images)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for i in range(num_images):\n",
    "        plt.subplot(1, num_images, i + 1)\n",
    "        plt.imshow(np.transpose(images[i], (1, 2, 0)))\n",
    "        plt.axis('off')\n",
    "        if titles:\n",
    "            plt.title(titles[i])\n",
    "    plt.show()\n",
    "\n",
    "# Select 5 random samples\n",
    "selected_samples = np.random.choice(len(train_dataset), 5, replace=False)\n",
    "\n",
    "# Fetch and display the selected samples\n",
    "for idx in selected_samples:\n",
    "    label, sketch, image, img_name,rand = train_dataset[idx]\n",
    "    print(\"Image Name:\", img_name)\n",
    "    print(\"Label:\", label)\n",
    "    show_images([image, sketch], titles=['Image', 'Sketch'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride):\n",
    "        super(CNNBlock, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels, out_channels, 4, stride, 1, bias=False, padding_mode=\"reflect\"\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels=10, features=[64, 128, 256, 512]):\n",
    "        super().__init__()\n",
    "        # in_channels = 10\n",
    "        self.initial = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                # in_channels * 2,\n",
    "                in_channels ,\n",
    "                features[0],\n",
    "                kernel_size=4,\n",
    "                stride=2,\n",
    "                padding=1,\n",
    "                padding_mode=\"reflect\",\n",
    "            ),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "\n",
    "        layers = []\n",
    "        in_channels = features[0]\n",
    "        for feature in features[1:]:\n",
    "            layers.append(\n",
    "                CNNBlock(in_channels, feature, stride=1 if feature == features[-1] else 2),\n",
    "            )\n",
    "            in_channels = feature\n",
    "\n",
    "        layers.append(\n",
    "            nn.Conv2d(\n",
    "                in_channels, 1, kernel_size=4, stride=1, padding=1, padding_mode=\"reflect\"\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        # print(\"x: \",x.shape)\n",
    "        # print(\"y: \",y.shape)\n",
    "        x = torch.cat([x, y], dim=1)\n",
    "        # print(\"x: \",x.shape)\n",
    "        # in_channels=3\n",
    "        x = self.initial(x)\n",
    "        # print(\"x: \",x.shape)\n",
    "        x = self.model(x)\n",
    "        # print(\"x: \",x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, down=True, act=\"relu\", use_dropout=False):\n",
    "        super(Block, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 4, 2, 1, bias=False, padding_mode=\"reflect\")\n",
    "            if down\n",
    "            else nn.ConvTranspose2d(in_channels, out_channels, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU() if act == \"relu\" else nn.LeakyReLU(0.2),\n",
    "        )\n",
    "\n",
    "        self.use_dropout = use_dropout\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.down = down\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return self.dropout(x) if self.use_dropout else x\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, in_channels=10, features=64): # 3 earlier\n",
    "        super().__init__()\n",
    "        self.initial_down = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, features, 4, 2, 1, padding_mode=\"reflect\"),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "        self.down1 = Block(features, features * 2, down=True, act=\"leaky\", use_dropout=False)\n",
    "        self.down2 = Block(\n",
    "            features * 2, features * 4, down=True, act=\"leaky\", use_dropout=False\n",
    "        )\n",
    "        self.down3 = Block(\n",
    "            features * 4, features * 8, down=True, act=\"leaky\", use_dropout=False\n",
    "        )\n",
    "        self.down4 = Block(\n",
    "            features * 8, features * 8, down=True, act=\"leaky\", use_dropout=False\n",
    "        )\n",
    "        self.down5 = Block(\n",
    "            features * 8, features * 8, down=True, act=\"leaky\", use_dropout=False\n",
    "        )\n",
    "        self.down6 = Block(\n",
    "            features * 8, features * 8, down=True, act=\"leaky\", use_dropout=False\n",
    "        )\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Conv2d(features * 8, features * 8, 4, 2, 1), nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.up1 = Block(features * 8, features * 8, down=False, act=\"relu\", use_dropout=True)\n",
    "        self.up2 = Block(\n",
    "            features * 8 * 2, features * 8, down=False, act=\"relu\", use_dropout=True\n",
    "        )\n",
    "        self.up3 = Block(\n",
    "            features * 8 * 2, features * 8, down=False, act=\"relu\", use_dropout=True\n",
    "        )\n",
    "        self.up4 = Block(\n",
    "            features * 8 * 2, features * 8, down=False, act=\"relu\", use_dropout=False\n",
    "        )\n",
    "        self.up5 = Block(\n",
    "            features * 8 * 2, features * 4, down=False, act=\"relu\", use_dropout=False\n",
    "        )\n",
    "        self.up6 = Block(\n",
    "            features * 4 * 2, features * 2, down=False, act=\"relu\", use_dropout=False\n",
    "        )\n",
    "        self.up7 = Block(features * 2 * 2, features, down=False, act=\"relu\", use_dropout=False)\n",
    "        self.final_up = nn.Sequential(\n",
    "            nn.ConvTranspose2d(features * 2, 3, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        d1 = self.initial_down(x)\n",
    "        d2 = self.down1(d1)\n",
    "        d3 = self.down2(d2)\n",
    "        d4 = self.down3(d3)\n",
    "        d5 = self.down4(d4)\n",
    "        d6 = self.down5(d5)\n",
    "        d7 = self.down6(d6)\n",
    "        bottleneck = self.bottleneck(d7)\n",
    "        up1 = self.up1(bottleneck)\n",
    "        up2 = self.up2(torch.cat([up1, d7], 1))\n",
    "        up3 = self.up3(torch.cat([up2, d6], 1))\n",
    "        up4 = self.up4(torch.cat([up3, d5], 1))\n",
    "        up5 = self.up5(torch.cat([up4, d4], 1))\n",
    "        up6 = self.up6(torch.cat([up5, d3], 1))\n",
    "        up7 = self.up7(torch.cat([up6, d2], 1))\n",
    "        last = self.final_up(torch.cat([up7, d1], 1))\n",
    "        # print(\"last: \",last.shape)\n",
    "        return last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc = Discriminator(in_channels=10).to(device)\n",
    "gen = Generator(in_channels=10, features=64).to(device)\n",
    "opt_disc = optim.Adam(disc.parameters(), lr=2e-4, betas=(0.5, 0.999),)\n",
    "opt_gen = optim.Adam(gen.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
    "g_scaler = torch.cuda.amp.GradScaler()\n",
    "d_scaler = torch.cuda.amp.GradScaler()\n",
    "bce = nn.BCEWithLogitsLoss()\n",
    "l1_loss = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 100\n",
    "L1_LAMBDA = 100\n",
    "\n",
    "# Set directory to save images\n",
    "save_dir = \"CGAN_paired\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "step = 0\n",
    "\n",
    "# Lists to store losses for plotting\n",
    "train_losses_D = []\n",
    "train_losses_G = []\n",
    "\n",
    "gen.train()\n",
    "disc.train()\n",
    "\n",
    "best_loss = float('inf')\n",
    "best_model_path = os.path.join(save_dir, \"best_model.pth\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "\n",
    "    # Initialize variables to accumulate losses over the epoch\n",
    "    epoch_train_loss_D = 0\n",
    "    epoch_train_loss_G = 0\n",
    "\n",
    "    for batch_idx, (labels, sketch,real,name,rand_labels) in enumerate(tqdm(dataloader)):\n",
    "        \n",
    "        \n",
    "        real = real.to(device)\n",
    "        noise= sketch.to(device)\n",
    "        labels = labels.to(device).long()\n",
    "        rand_labels = rand_labels.to(device).long()\n",
    "        # print(\"labels : \",labels)\n",
    "        # print(\"rand_labels: \",rand_labels)\n",
    "        img_size=256\n",
    "        \n",
    "        # Train Discriminator\n",
    "        with torch.cuda.amp.autocast():\n",
    "            embed_disc=nn.Embedding(7, img_size*img_size).to(device)\n",
    "            embedding_disc=embed_disc(labels) # Real Label given to discriminator\n",
    "            embedding_disc=embedding_disc.view(labels.shape[0],7,img_size,img_size)\n",
    "\n",
    "            embed_gen = nn.Embedding(7, 1).to(device)\n",
    "            embedding_gen = embed_gen(rand_labels) # Random Label to generator\n",
    "            \n",
    "            # print(\"embedding_gen: \",embedding_gen.shape)\n",
    "            embedding_gen = embedding_gen.unsqueeze(3)\n",
    "            upsampled_embedding_gen = F.interpolate(embedding_gen, size=(img_size,img_size), mode='nearest')\n",
    "            real_gen = torch.cat([noise, upsampled_embedding_gen], dim=1)\n",
    "            \n",
    "            y_fake = gen(real_gen) # Generate fake images\n",
    "\n",
    "            # print(\"y_fake: \",y_fake.shape)\n",
    "\n",
    "            D_real = disc(real,embedding_disc) # Discriminator with real images\n",
    "            D_real_loss = bce(D_real, torch.ones_like(D_real)) # Loss with real images\n",
    "\n",
    "            # print(\"y_fake: \",y_fake.shape)\n",
    "            # print(\"embedding_disc: \",embedding_disc.shape)\n",
    "            D_fake = disc(y_fake.detach(),embedding_disc) # Discriminator with fake images\n",
    "            D_fake_loss = bce(D_fake, torch.zeros_like(D_fake)) # Loss with fake images\n",
    "            \n",
    "            D_loss = (D_real_loss + D_fake_loss) / 2\n",
    "            disc.zero_grad()\n",
    "            d_scaler.scale(D_loss).backward(retain_graph=True)\n",
    "            d_scaler.step(opt_disc)\n",
    "            d_scaler.update()\n",
    "\n",
    "        # Train Generator\n",
    "            D_fake = disc(y_fake,embedding_disc)\n",
    "            G_fake_loss = bce(D_fake, torch.ones_like(D_fake))\n",
    "            # print(\"y_fake: \",y_fake.shape)\n",
    "            # print(\"real: \",real.shape)\n",
    "            L1 = l1_loss(y_fake, real) * L1_LAMBDA\n",
    "            G_loss = G_fake_loss + L1\n",
    "\n",
    "            opt_gen.zero_grad()\n",
    "            g_scaler.scale(G_loss).backward(retain_graph=True)\n",
    "            g_scaler.step(opt_gen)\n",
    "            g_scaler.update()\n",
    "\n",
    "        if batch_idx % 50 == 0:\n",
    "            with torch.no_grad():\n",
    "                fake = gen(real_gen)\n",
    "\n",
    "               # Create grid containing both real and fake images\n",
    "                img_grid = torch.cat((real[:8], fake[:8], noise[:8]), dim=0)\n",
    "               \n",
    "                # Make grid for visualization\n",
    "                img_grid = torchvision.utils.make_grid(img_grid, nrow=8, normalize=True)\n",
    "\n",
    "                # fid = FrechetInceptionDistance(feature=64)\n",
    "                # fid.update(real, real=True)\n",
    "                # fid.update(fake, real=False)\n",
    "\n",
    "                # Save the generated fake images locally\n",
    "                torchvision.utils.save_image(img_grid, os.path.join(save_dir, f\"epoch_{epoch}_batch_{batch_idx}.png\"))\n",
    "\n",
    "            print(\n",
    "                f\"Epoch [{epoch}/{NUM_EPOCHS}] Batch {batch_idx}/{len(dataloader)} \\\n",
    "                  Loss D: {D_loss:.4f}, loss G: {G_loss:.4f}\"\n",
    "            )\n",
    "\n",
    "            step += 1\n",
    "\n",
    "        # Accumulate losses\n",
    "        epoch_train_loss_D += D_loss.item()\n",
    "        epoch_train_loss_G += G_loss.item()\n",
    "\n",
    "    # Calculate average training losses for the epoch\n",
    "    avg_train_loss_D = epoch_train_loss_D / len(dataloader)\n",
    "    avg_train_loss_G = epoch_train_loss_G / len(dataloader)\n",
    "\n",
    "    # Store the average training losses for plotting\n",
    "    train_losses_D.append(avg_train_loss_D)\n",
    "    train_losses_G.append(avg_train_loss_G)\n",
    "\n",
    "\n",
    "    # Save the model if it has the best performance on the training set\n",
    "    if avg_train_loss_G < best_loss:\n",
    "        best_loss = avg_train_loss_G\n",
    "        torch.save(gen.state_dict(), best_model_path)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the training curves\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses_D, label='Train D loss')\n",
    "plt.plot(train_losses_G, label='Train G loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Curve')\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(save_dir, \"training_curve.png\"))\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "idf1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
